{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from dataclean import *\n",
    "# from define_classes import *\n",
    "# # from data_processing import *\n",
    "# from pca import *\n",
    "# from csv_clean import *\n",
    "# from polynomial_regression import *\n",
    "# from pipe import run_models\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the number of classes for different wine quality points threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible thresdholds for classification: [3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv', index_col=0)\n",
    "print('Possible thresdholds for classification:',np.sort(data['quality'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'cat'\n",
    "n_c = 3 # bad, average, good wine\n",
    "thres = [4,6,7]\n",
    "value = 'quality'\n",
    "\n",
    "# import define_classes\n",
    "\n",
    "cd = CleanData()\n",
    "dt = cd.DefineTarget(data)\n",
    "\n",
    "data = dt.get_classes(target, n_c, thres, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4974\n",
       "2    1277\n",
       "0     246\n",
       "Name: cat, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fixedacidity', 'volatileacidity', 'citricacid', 'residualsugar',\n",
       "       'chlorides', 'freesulfurdioxide', 'totalsulfurdioxide', 'density', 'pH',\n",
       "       'sulphates', 'alcohol', 'quality', 'type', 'cat'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client Features Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our client doesn't gather the same data as contained in our innitial sample, to train the model we will include only the information that is available for the company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_data = ['alcohol', 'fixedacidity', 'pH', 'volatileacidity',\n",
    "       'freesulfurdioxide', 'totalsulfurdioxide', 'residualsugar', 'type','cat']\n",
    "\n",
    "# rewrite csv with the target variable\n",
    "data = data[client_data]\n",
    "data.to_csv('data_client.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see a large inbalance in the very good(class 3) and bad wines (class 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import recall_score, classification_report, accuracy_score, precision_score, precision_recall_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from ordinal import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from yellowbrick.classifier import PrecisionRecallCurve\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Data\n",
    "\n",
    "df = pd.read_csv('data_client.csv', index_col=0)\n",
    "y = 'cat'\n",
    "df_mm = cd.scale(df)\n",
    "X_train, X_test, y_train, y_test = cd.split(df_mm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['alcohol', 'fixedacidity', 'pH', 'volatileacidity', 'freesulfurdioxide',\n",
       "       'totalsulfurdioxide', 'residualsugar', 'type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5197,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the Classes : SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE(sampling_strategy='not majority', k_neighbors=3)\n",
    "\n",
    "X_train_s, y_train_s = smt.fit_sample(np.asarray(X_train), np.asarray(y_train))\n",
    "\n",
    "col = list(X_train.columns)\n",
    "\n",
    "X_train_smote = pd.DataFrame(X_train_s, columns=col)\n",
    "y_train_smote = pd.DataFrame(y_train_s, columns=['cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model : Ordinal Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinal variables are those whose value exists on an arbitrary scale where only the relative ordering between different values is significant. In our case 0 to 3 is the order definining the incremental quality of wine. Therefore the estimation should take into account that our variables are 'order-dependent'.    \n",
    "A way to tackle this type of classes (ordered) is using the 'OrdinalClassifier' class available for python in GitHub. This will create binary vectors for each class. This can be applied to classifiers that generate classes probability, or in python what we know as 'predict_proba'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bin = label_binarize(y_train_smote, classes=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RunModels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-4003eb7d6b21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRunModels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrdinalClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1e9\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multinomial'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RunModels' is not defined"
     ]
    }
   ],
   "source": [
    "mdl = RunModels()\n",
    "clf = mdl.OrdinalClassifier(LogisticRegression(C =1e9 ,solver='newton-cg',fit_intercept=True, multi_class='multinomial'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.69      0.24        55\n",
      "           1       0.88      0.48      0.62       992\n",
      "           2       0.39      0.77      0.52       253\n",
      "\n",
      "    accuracy                           0.54      1300\n",
      "   macro avg       0.47      0.65      0.46      1300\n",
      "weighted avg       0.75      0.54      0.58      1300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_prob = clf.predict_proba(X_test)[:,1]\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: \n",
    " - High precission for only the average wine class. High recall on the classes with low precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(multi_class='multinomial', random_state=1, class_weight='balanced')\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=1, class_weight='balanced' )\n",
    "clf3 = GaussianNB()\n",
    "clf4 = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "clf5 = SVC(kernel=\"linear\", class_weight=\"balanced\", C=1.0, decision_function_shape = 'ovo')\n",
    "clf6 = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf1 = VotingClassifier(estimators=[ ('lr', clf1), ('rf', clf2), ('gnb', clf3), ('knn', clf4), ('svc', clf5)], voting='hard')\n",
    "eclf1 = eclf1.fit(X_train, y_train)\n",
    "y_vt = eclf1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vot = eclf1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_vt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_vt)\n",
    "cnf_matrix\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap='YlGnBu' ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline and GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8de44c9418dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mg_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_smote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_smote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'run_models' is not defined"
     ]
    }
   ],
   "source": [
    "g_p, b_e, y_pred_p = run_models(X_train_smote, y_train_smote, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Results:\n",
    " \n",
    "  - RandomForest seems to perform the best for the three classes\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = PrecisionRecallCurve(b_e[1], \n",
    "                    per_class=True, iso_f1_curves=True,\n",
    "                    fill_area=False, micro=False, classes=['bad','average','good']\n",
    "                )\n",
    "\n",
    "viz.fit(X_train_smote, y_train_smote)\n",
    "viz.score(X_test, y_test)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred_p[1])\n",
    "cnf_matrix\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap='YlGnBu' ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: Voting vs Gridsearch\n",
    "- Pipeline and GridsearchCV shows better predictive results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = list(b_e[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelrf = m\n",
    "modelrf.fit(X_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = modelrf.predict(X_test)\n",
    "\n",
    "importances = modelrf.feature_importances_\n",
    "\n",
    "dt_feature_importance = [(X_train_smote.columns[i[0]], i[1]) for i in list(enumerate(importances))]\n",
    "dt_feature_importance = sorted(dt_feature_importance, key = lambda x: x[1], reverse = True)[:5]\n",
    "fig = plt.figure(figsize = (8,5))\n",
    "plt.bar([i[0] for i in dt_feature_importance], [i[1] for i in dt_feature_importance], color='skyblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fit = modelrf.predict(X_test)\n",
    "print(classification_report(y_test, y_fit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Client's Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelrf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                       criterion='gini', max_depth=None, max_features='auto',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
    "                       n_jobs=None, oob_score=False, random_state=None,\n",
    "                       verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelrf.fit(X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr = pd.read_csv('wine_reports.csv')\n",
    "\n",
    "lea = wr.iloc[:,:8].fillna(0)\n",
    "\n",
    "client_df = d.scale(lea)\n",
    "\n",
    "modelrf.predict(client_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance: PCA - 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data_client.csv', index_col=0)\n",
    "target = 'cat'\n",
    "n_com = 5\n",
    "p = PcaAnalysis()\n",
    "pca_df = p.pca_features(df, target, n_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "pca = pca_df.join(df['cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Data\n",
    "d = DataClean()\n",
    "pca_scale = d.scale(pca)\n",
    "X_train_p, X_test_p, y_train_p, y_test_p = d.split(pca_scale,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_scale.groupby('cat').count().iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebalancing Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE(sampling_strategy='not majority', k_neighbors=3)\n",
    "X_train_s, y_train_s = smt.fit_sample(np.asarray(X_train_p), np.asarray(y_train_p))\n",
    "\n",
    "col = list(X_train_p.columns)\n",
    "\n",
    "X_train_p = pd.DataFrame(X_train_s, columns=col)\n",
    "y_train_p= pd.DataFrame(y_train_s, columns=['cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_p_p, b_e_p, y_pred_p_p = run_models(X_train_p, y_train_p, X_test_p, y_test_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "    - Compared to the previous estimation PCA components does not improve significantly the performance. \n",
    "    - Random forest continues to perform the best. Small improvement from non-PCA model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRECISION-RECALL Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = PrecisionRecallCurve(b_e_p[1], \n",
    "                    per_class=True, iso_f1_curves=True,\n",
    "                    fill_area=False, micro=False, classes=['bad','average','good']\n",
    "                )\n",
    "\n",
    "pr.fit(X_train_p, y_train_p)\n",
    "pr.score(X_test_p, y_test_p)\n",
    "pr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
    "                                        class_weight=None, criterion='gini',\n",
    "                                        max_depth=None, max_features='auto',\n",
    "                                        max_leaf_nodes=None, max_samples=None,\n",
    "                                        min_impurity_decrease=0.0,\n",
    "                                        min_impurity_split=None,\n",
    "                                        min_samples_leaf=1, min_samples_split=2,\n",
    "                                        min_weight_fraction_leaf=0.0,\n",
    "                                        n_estimators=1000, n_jobs=None,\n",
    "                                        oob_score=False, random_state=None,\n",
    "                                        verbose=0, warm_start=False)\n",
    "model.fit(X_train_p, y_train_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_p)\n",
    "\n",
    "importances = model.feature_importances_\n",
    "\n",
    "y_pred = model.predict(X_test_p)\n",
    "\n",
    "importances = model.feature_importances_\n",
    "\n",
    "dt_feature_importance = [(X_train_p.columns[i[0]], i[1]) for i in list(enumerate(importances))]\n",
    "dt_feature_importance = sorted(dt_feature_importance, key = lambda x: x[1], reverse = True)[:5]\n",
    "fig = plt.figure(figsize = (8,5))\n",
    "plt.bar([i[0] for i in dt_feature_importance], [i[1] for i in dt_feature_importance], color='skyblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "\n",
    "- Free and total SO2 (sulfur dioxide) as we could expect shows the hights VIF. We will remove total sulfure and test our models agian in a separate notebook.\n",
    "- After removing the feature in a separete notebook we could see that there is no improvement in our accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
